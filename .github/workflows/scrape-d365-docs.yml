name: Scrape D365 Documentation

permissions:
  contents: write

on:
  repository_dispatch:
    types: [scrape-docs]
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Install dependencies
        run: npm install

      - name: Start server in background
        run: |
          node server.js > server.log 2>&1 & 
          echo $! > server.pid
          sleep 5

      - name: Wait for server readiness
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:3000/health 2>/dev/null || curl -f http://localhost:3000 2>/dev/null; then
              echo "Server is ready!"
              break
            fi
            echo "Waiting for server... ($i/30)"
            sleep 2
          done

      - name: Trigger scraping → create latest CSV
        run: |
          curl -s http://localhost:3000/scrape-all -o latest-scrape.csv

      - name: Verify CSV exists
        run: |
          if [ -f latest-scrape.csv ]; then
            echo "CSV generated successfully."
          else
            echo "CSV missing!"
            exit 1
          fi

      - name: Stop server
        if: always()
        run: |
          if [ -f server.pid ]; then kill $(cat server.pid) || true; fi
          pkill -f "node server.js" || true

      - name: Prepare history folder
        run: |
          mkdir -p history
          cp latest-scrape.csv history/scrape_$(date +"%Y_%m_%d_%H_%M").csv

      - name: Keep only last 5 history files
        run: |
          cd history
          ls -t | sed -e '1,5d' | xargs -r rm -f

      - name: Generate comparison report
        run: |
          if [ $(ls history | wc -l) -ge 2 ]; then
            NEW=latest-scrape.csv
            OLD=$(ls -t history | sed -n '2p')

            echo "### Added URLs" > comparison.txt
            comm -23 <(cut -d',' -f2 "$NEW" | sort) <(cut -d',' -f2 "history/$OLD" | sort) >> comparison.txt

            echo "" >> comparison.txt
            echo "### Removed URLs" >> comparison.txt
            comm -13 <(cut -d',' -f2 "$NEW" | sort) <(cut -d',' -f2 "history/$OLD" | sort) >> comparison.txt

            echo "" >> comparison.txt
            echo "### Changed LastUpdated" >> comparison.txt
            join -t',' -1 1 -2 1 \
              <(cut -d',' -f2,3 "$NEW" | sed 's/"//g' | sort -t',' -k1,1) \
              <(cut -d',' -f2,3 "history/$OLD" | sed 's/"//g' | sort -t',' -k1,1) \
              | awk -F',' '{
                  url=$1;
                  new=$2;
                  old=$3;
                  if (new != old && new != "" && old != "" && new != "UNKNOWN" && old != "UNKNOWN")
                    printf "%s   %s → %s\n", url, old, new;
                }' >> comparison.txt
          else
            echo "Not enough history files to compare." > comparison.txt
          fi

      - name: Install zip utility
        run: sudo apt-get install -y zip

      - name: Create release ZIP
        run: |
          zip -r d365-scrape-results.zip latest-scrape.csv comparison.txt history/

      - name: Create/Update GitHub Release (latest)
        uses: softprops/action-gh-release@v1
        with:
          tag_name: latest
          name: Latest D365 Scrape Results
          body: "Packaged ZIP of latest scrape, comparison, and history."
          files: d365-scrape-results.zip
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
